local server = {
  _VERSION = "0.1",
  _NAME = "mapreduce.server",
}

local utils  = require "mapreduce.utils"
local task   = require "mapreduce.task"
local cnn    = require "mapreduce.cnn"

local DEFAULT_HOSTNAME = utils.DEFAULT_HOSTNAME
local DEFAULT_IP = utils.DEFAULT_IP
local DEFAULT_DATE = utils.DEFAULT_DATE
local STATUS = utils.STATUS
local TASK_STATUS = utils.TASK_STATUS
local grp_tmp_dir = utils.GRP_TMP_DIR
local red_job_tmp_dir = utils.RED_JOB_TMP_DIR
local escape = utils.escape
local serialize_table_ipairs = utils.serialize_table_ipairs
local make_job = utils.make_job
local gridfs_lines_iterator = utils.gridfs_lines_iterator

-- PRIVATE FUNCTIONS AND METHODS

local function compute_real_time(db, ns)
  local result_min = db:mapreduce(ns, [[
function() { emit(0, this.started_time) } ]],
               [[
function(k,v) {
  var min=v[0];
  for (var i=1; i<v.length; ++i)
  if (v[i]<min) min=v[i];
  return min;
}]])
  local result_max = db:mapreduce(ns, [[
function() { emit(0, this.written_time) } ]],
               [[
function(k,v) {
  var max=v[0];
  for (var i=1; i<v.length; ++i)
  if (v[i]>max) max=v[i];
  return max;
}]])
  return result_max.results[1].value - result_min.results[1].value
end

local function compute_cpu_time(db, ns)
  local result = db:mapreduce(ns, [[
function() { emit(0, this.cpu_time) } ]],
                                  [[
function(k,v) { return Array.sum(v); }]])
  return result.results[1].value
end

-- returns a coroutine.wrap which returns true until all tasks are finished
local function make_task_coroutine_wrap(self,ns)
  local db = self.cnn:connect()
  local N = db:count(ns)
  return coroutine.wrap(function()
                          repeat
                            local db = self.cnn:connect()
                            local M = db:count(ns, { status = STATUS.WRITTEN })
                            if M then
                              io.stderr:write(string.format("\r\t %6.1f %% ",
                                                            M/N*100))
                              io.stderr:flush()
                            end
                            if not M or M < N then coroutine.yield(true) end
                          until M == N
                          io.stderr:write("\n")
                        end)
end

-- removes all the tasks which are not WRITTEN
local function remove_pending_tasks(db,ns)
  return db:remove(ns,
                   { ["$or"] = { { status = STATUS.BROKEN,  },
                                 { status = STATUS.WAITING  },
                                 { status = STATUS.FINISHED },
                                 { status = STATUS.RUNNING  }, } },
                   false)
end

-- insert jobs in mongo db and returns a coroutine ready to be executed as an
-- iterator
local function server_prepare_map(self)
  local count = 0
  local db = self.cnn:connect()
  local map_jobs_ns = self.task:get_map_jobs_ns()
  remove_pending_tasks(db, map_jobs_ns)
  -- create map tasks in mongo database
  local f = self.taskfn.func
  local keys_check = {}
  for key,value in coroutine.wrap(f) do
    count = count + 1
    assert(tostring(key), "taskfn must return a string key")
    assert(not keys_check[key], string.format("Duplicate key: %s", key))
    keys_check[key] = true
    -- FIXME: check how to process task keys which are defined by a previously
    -- broken execution and didn't belong to the current task execution
    assert( db:insert(map_jobs_ns, make_job(key,value)) )
  end
  self.task:set_task_status(TASK_STATUS.MAP)
  -- this coroutine WAITS UNTIL ALL MAPS ARE DONE
  return make_task_coroutine_wrap(self, map_jobs_ns),count
end

-- insert the job in the mongo db and returns a coroutine
local function server_prepare_reduce(self)
  local db     = self.cnn:connect()
  local gridfs = self.cnn:gridfs()
  local dbname = self.cnn:get_dbname()
  local map_results_ns = self.task:get_map_results_ns()
  local red_jobs_ns = self.task:get_red_jobs_ns()
  remove_pending_tasks(db, red_jobs_ns)
  -- list the filenames generated by mappers in order to create the reduce jobs
  local match_str = string.format("^%s.*P.*M.*$",grp_tmp_dir)
  local filenames = {}
  local list = gridfs:list({ filename = { ["$regex"] = match_str } })
  local part_keys = {}
  for obj in list:results() do
    local filename = obj.filename
    -- sanity check
    assert(filename:match(match_str))
    -- create reduce jobs in mongo database, from partitioned space
    local part_key = assert(tonumber(filename:match("^.*.P([^%.]+)%.M[^%.]*$")))
    part_keys[part_key] = true
  end
  local count=0
  for part_key,_ in pairs(part_keys) do
    count = count + 1
    local value = {
      file   = string.format("%s/%s.P%d", grp_tmp_dir, map_results_ns, part_key),
      result = string.format("%s.P%d", self.result_ns, part_key),
    }
    self.cnn:annotate_insert(red_jobs_ns, make_job(part_key, value))
  end
  self.cnn:flush_pending_inserts(0)
  self.task:set_task_status(TASK_STATUS.REDUCE)
  -- this coroutine WAITS UNTIL ALL REDUCES ARE DONE
  return make_task_coroutine_wrap(self, red_jobs_ns),count
end

local function server_drop_collections(self)
  local db = self.cnn:connect()
  local dbname = self.cnn:get_dbname()
  -- drop all the collections
  for _,name in ipairs(db:get_collections(dbname)) do
    db:drop_collection(name)
  end
  local gridfs = self.cnn:gridfs()
  local list = gridfs:list()
  for v in list:results() do
    gridfs:remove_file(v.filename)
  end
end

-- finalizer for the map-reduce process
local function server_final(self)
  -- FIXME: self.result_ns could contain especial characters, it will be
  -- necessary to escape them
  local match_str = string.format("^%s",self.result_ns)
  local gridfs = self.cnn:gridfs()
  local files = gridfs:list({ filename = { ["$regex"] = match_str } })
  local current_file
  local lines_iterator
  -- iterator which is given to final function, allows to traverse all the
  -- results by pairs key,value
  local pair_iterator = function()
    local line
    repeat
      if lines_iterator then
        line = lines_iterator()
      end
      if not line then
        current_file = files:next()
        if current_file then
          assert(current_file.filename:match(match_str))
          lines_iterator = gridfs_lines_iterator(gridfs,current_file.filename)
        end
      end
    until current_file == nil or line ~= nil
    if line then
      return load(line)()
    end
  end
  -- the reply could be: false/nil, true, "loop"
  local reply = self.finalfn.func(pair_iterator)
  local remove_all = (reply == true) or (reply == "loop")
  if reply ~= "loop" and reply ~= true and reply ~= false and reply ~= nil then
    io.stderr:write("# WARNING!!! INCORRECT FINAL RETURN: " ..
                      tostring(reply) .. "\n")
  end
  -- drop collections, except reduce result and task status
  local db = self.cnn:connect()
  --
  local task = self.task
  if reply == "loop" then
    io.stderr:write("# LOOP again\n")
    db:drop_collection(task:get_map_jobs_ns())
    db:drop_collection(task:get_red_jobs_ns())
  else
    self.finished = true
    task:set_task_status(TASK_STATUS.FINISHED)
  end
  local gridfs = self.cnn:gridfs()
  local list = gridfs:list()
  for v in list:results() do
    if not v.filename:match(match_str) or remove_all then
      gridfs:remove_file(v.filename)
    end
  end
end

-- SERVER METHODS
local server_methods = {}

-- configures the server with the script string
function server_methods:configure(params)
  self.configured           = true
  self.configuration_params = params
  self.task_args            = params.task_args
  self.map_args             = params.map_args
  self.reduce_args          = params.reduce_args
  self.final_args           = params.final_args
  local dbname = self.dbname
  local taskfn,mapfn,reducefn,finalfn
  local scripts = {}
  self.result_ns = params.result_ns or "result"
  assert(params.taskfn and params.mapfn and params.partitionfn and params.reducefn,
         "Fields taskfn, mapfn, partitionfn and reducefn are mandatory")
  for _,name in ipairs{ "taskfn", "mapfn", "partitionfn", "reducefn", "finalfn" } do
    assert(params[name] and type(params[name]) == "string",
           string.format("Needs a %s module", name))
    local aux = require(params[name])
    assert(type(aux) == "table",
           string.format("Module %s must return a table",
                         name))
    assert(aux.func,
           string.format("Module %s must return a table with the field func",
                         name))
    assert(aux.init or not params[ name:gsub("fn","_args") ],
           string.format("When args are given, a init function is needed: %s",
                         name))
    scripts[name] = params[name]
  end
  local db = self.cnn:connect()
  --
  self.taskfn = require(scripts.taskfn)
  if scripts.finalfn then
    self.finalfn = require(scripts.finalfn)
  else
    self.finalfn = { func = function() end }
  end
  if self.taskfn.init then self.taskfn.init(self.task_args) end
  if self.finalfn.init then self.finalfn.init(self.final_args) end
  self.mapfn = params.mapfn
  self.reducefn = params.reducefn
end

-- makes all the map-reduce process, looping into the coroutines until all tasks
-- are done
function server_methods:loop()
  local it = 0
  repeat
    local skip_map,initialize=false,true
    if it == 0 then
      -- in the first iteration, we check if the task is a new fresh execution
      -- or if a previous broken task exists
      self.task:update()
      if self.task:has_status() then
        local status = self.task:get_task_status()
        if status == TASK_STATUS.REDUCE then
          -- if the task was in reduce state, skip map jobs and re-run reduce
          io.stderr:write("# WARNING: TRYING TO RESTORE A BROKEN TASK\n")
          skip_map   = true
          initialize = false
        elseif status == TASK_STATUS.FINISHED then
          -- if the task was finished, therefore it is a shit, drop old data
          server_drop_collections(self)
        else
          -- otherwise, the task is in WAIT or MAP states, try to restore from
          -- there
          initialize = false
        end
      end -- if task has status
    end -- if it == 0
    if initialize then
      -- count one iteration
      it = it+1
      -- create task object
      self.task:create_collection(TASK_STATUS.WAIT,
                                  self.configuration_params, it)
    else
      it = self.task:get_iteration()
      self.task:create_collection(self.task:get_task_status(),
                                  self.configuration_params,
                                  it)
    end
    io.stderr:write(string.format("# Iteration %d\n", it))
    local time = os.time()
    self.task:insert_started_time(time)
    if not skip_map then
      -- MAP EXECUTION
      io.stderr:write("# \t Preparing Map\n")
      local do_map_step,map_count = server_prepare_map(self)
      collectgarbage("collect")
      io.stderr:write(string.format("# \t Map execution, size= %d\n",
                                    map_count))
      while do_map_step() do
        utils.sleep(utils.DEFAULT_SLEEP)
        collectgarbage("collect")
      end
    end
    local db = self.cnn:connect()
    local map_count = db:count(self.task:get_map_jobs_ns())
    -- REDUCE EXECUTION
    collectgarbage("collect")
    io.stderr:write("# \t Preparing Reduce\n")
    local do_reduce_step = server_prepare_reduce(self)
    local db = self.cnn:connect()
    local red_count = db:count(self.task:get_red_jobs_ns())
    collectgarbage("collect")
    io.stderr:write(string.format("# \t Reduce execution, num_files= %d  size= %d\n",
                                  red_count * map_count, red_count))
    while do_reduce_step() do
      utils.sleep(utils.DEFAULT_SLEEP)
      collectgarbage("collect")
    end
    -- TIME
    local end_time = os.time()
    local total_time = end_time - time
    self.task:insert_finished_time(end_time)
    -- FINAL EXECUTION
    io.stderr:write("# \t Final execution\n")
    collectgarbage("collect")
    server_final(self)
    --
    -- STATISTICS
    local map_sum_cpu_time = compute_cpu_time(db, self.task:get_map_jobs_ns())
    local red_sum_cpu_time = compute_cpu_time(db, self.task:get_red_jobs_ns())
    local map_real_time    = compute_real_time(db, self.task:get_map_jobs_ns())
    local red_real_time    = compute_real_time(db, self.task:get_red_jobs_ns())

    io.stderr:write(string.format("#   Map sum(cpu_time)    %f\n",
                                  map_sum_cpu_time))
    io.stderr:write(string.format("#   Reduce sum(cpu_time) %f\n",
                                  red_sum_cpu_time))
    io.stderr:write(string.format("# Sum(cpu_time)          %f\n",
                                  map_sum_cpu_time + red_sum_cpu_time))
    io.stderr:write(string.format("#   Map real time    %d\n", map_real_time))
    io.stderr:write(string.format("#   Reduce real time %d\n", red_real_time))
    io.stderr:write(string.format("# Real time          %d\n",
                                  map_real_time + red_real_time))
    --
    self.task:insert{
      map_sum_cpu_time = map_sum_cpu_time,
      red_sum_cpu_time = red_sum_cpu_time,
      total_sum_cpu_time = map_sum_cpu_time + red_sum_cpu_time,
      map_real_time = map_real_time,
      red_real_time = red_real_time,
      total_real_time = map_real_time + red_real_time,
      iteration_time = total_time,
    }
    --
    io.stderr:write(string.format("# Iteration time %d\n", total_time))
  until self.finished
end

-- SERVER METATABLE
local server_metatable = { __index = server_methods }

server.new = function(connection_string, dbname, auth_table)
  local cnn_obj = cnn(connection_string, dbname, auth_table)
  local obj = {
    cnn  = cnn_obj,
    task = task(cnn_obj),
  }
  setmetatable(obj, server_metatable)
  return obj
end

----------------------------------------------------------------------------
------------------------------ UNIT TEST -----------------------------------
----------------------------------------------------------------------------
server.utest = function(connection_string, dbname, auth_table)
  -- check serialization of map results
  local f = {
    write = function(self,str)
      self.tbl = self.tbl or {}
      table.insert(self.tbl,str)
    end,
    concat = function(self) return table.concat(self.tbl or {}) end,
  }
  -- FIXME: that is totally broken now
  utils.serialize_sorted_by_lines(f,{
                                    KEY1 = {1,1,1,1,1},
                                    KEY2 = {1,1,1},
                                    KEY3 = {1},
                                    KEY4 = { "hello\nworld" }
                                    })
  local result = [[return "KEY1",{1,1,1,1,1}
return "KEY2",{1,1,1}
return "KEY3",{1}
return "KEY4",{"hello\nworld"}
]]
  assert(f:concat() == result)
  -- check lines iterator over gridfs
  local cnn = cnn("localhost", "tmp")
  local db = cnn:connect()
  local gridfs = cnn:gridfs()
  local tmpname = os.tmpname()
  local f = io.open(tmpname, "w")
  f:write("first line\n")
  f:write("second line\n")
  f:write("third line\n")
  -- a large line, through multiple chunks
  for i=1,2^22 do
    f:write(string.format("a%d",i))
  end
  f:write("\n")
  f:close()
  gridfs:remove_file(tmpname)
  gridfs:store_file(tmpname,tmpname)
  local f = io.open(tmpname)
  for g_line in gridfs_lines_iterator(gridfs,tmpname) do
    local f_line = f:read("*l")
    assert(g_line == f_line)
  end
  os.remove(tmpname)
  -- check merge over several filenames
  local N=3
  local list_tmpnames = {} for i=1,N do list_tmpnames[i] = os.tmpname() end
  local list_files = {}
  for i,name in ipairs(list_tmpnames) do list_files[i]=io.open(name,"w") end
  -- FILE 1
  list_files[1]:write('return "a",{1,1,1}\n')
  list_files[1]:write('return "b",{1}\n')
  -- FILE 2
  list_files[2]:write('return "a",{1}\n')
  list_files[2]:write('return "c",{1,1,1,1,1,1,1,1,1}\n')
  -- FILE 3
  list_files[3]:write('return "a",{1,1}\n')
  list_files[3]:write('return "c",{1}\n')
  list_files[3]:write('return "d",{1,1,1,1,1,1}\n')
  --
  for i,f in ipairs(list_files) do
    f:close()
    gridfs:remove_file(list_tmpnames[i])
    gridfs:store_file(list_tmpnames[i], list_tmpnames[i])
    os.remove(list_tmpnames[i])
  end
  merge_gridfs_files(cnn, db, gridfs, list_tmpnames, 'result', 'tmp.result2')
  local lines = {
    'return "a",{1,1,1,1,1,1}',
    'return "c",{1,1,1,1,1,1,1,1,1,1}',
    'return "d",{1,1,1,1,1,1}',
  }
  for line in gridfs_lines_iterator(gridfs, "result") do
    assert(line == table.remove(lines,1))
  end
  for v in db:query('tmp.result2'):results() do
    assert(v._id == "b")
    assert(v.value == 1)
  end
end

------------------------------------------------------------------------------

return server
